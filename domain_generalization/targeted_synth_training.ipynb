{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import trange, tqdm\n",
    "import wandb\n",
    "import argparse\n",
    "\n",
    "\n",
    "from main import get_model\n",
    "import agent_net\n",
    "from visda17 import get_visda_dataloaders, downsample_dataset\n",
    "from utils import AverageMeter\n",
    "from gumbel_softmax import *\n",
    "\n",
    "# load the resnet26 model (optionally with IN weights)\n",
    "# make a training dataset with synthetic VisDA data\n",
    "# make a validation dataset with real ViSDA test\n",
    "# do spottune training (or optionally turn it off)\n",
    "\n",
    "def train_epoch(train_loader, net, agent=None):\n",
    "    net.train()\n",
    "\n",
    "    total_step = len(train_loader)\n",
    "    train_acc = AverageMeter()\n",
    "    train_loss = AverageMeter()\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader)):\n",
    "        images, labels = batch   \n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            images, labels = images.cuda(non_blocking=True), labels.cuda(non_blocking=True)  \n",
    "\n",
    "        if agent:\n",
    "            probs = agent(images)\n",
    "            action = gumbel_softmax(probs.view(probs.size(0), -1, 2))\n",
    "            policy = action[:,:,1]\n",
    "        else:\n",
    "            policy = None\n",
    "\n",
    "        outputs = net.forward(images, policy)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = predicted.eq(labels.data).cpu().sum()\n",
    "        train_acc.update(correct.item()*100 / (labels.size(0)+0.0), labels.size(0))\n",
    "\n",
    "        # Loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        loss = criterion(outputs, labels)\n",
    "        train_loss.update(loss.item(), labels.size(0))\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            wandb.log({'train_acc': train_acc.val, 'train_loss': train_loss.val})\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        if agent:\n",
    "            agent_optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()  \n",
    "\n",
    "        optimizer.step()\n",
    "        if agent:\n",
    "            agent_optimizer.step()\n",
    "\n",
    "def validate_epoch(val_loader, net, agent=None):\n",
    "    net.eval()\n",
    "\n",
    "    val_acc = AverageMeter()\n",
    "\n",
    "    for i, batch in enumerate(tqdm(val_loader)):\n",
    "        images, labels = batch\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            images, labels = images.cuda(non_blocking=True), labels.cuda(non_blocking=True)  \n",
    "\n",
    "        outputs = net.forward(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = predicted.eq(labels.data).cpu().sum()\n",
    "        val_acc.update(correct.item()*100 / (labels.size(0)+0.0), labels.size(0))\n",
    "\n",
    "\n",
    "    wandb.log({'val_acc': val_acc.avg})\n",
    "\n",
    "def setup_network(net):\n",
    "    # freeze the original blocks\n",
    "    flag = True\n",
    "    for name, m in net.named_modules():\n",
    "        if isinstance(m, nn.Conv2d) and 'parallel_blocks' not in name:\n",
    "            if flag is True:\n",
    "                flag = False\n",
    "            else:\n",
    "                m.weight.requires_grad = False\n",
    "\n",
    "    # Display info about frozen conv layers\n",
    "    conv_layers_finetune = [x[0] for x in net.named_modules() if isinstance(x[1], nn.Conv2d) and x[1].weight.requires_grad]\n",
    "    conv_layers_frozen = [x[0] for x in net.named_modules() if isinstance(x[1], nn.Conv2d) and not x[1].weight.requires_grad]\n",
    "\n",
    "    print(f\"Finetuning ({len(conv_layers_finetune)}) conv layers:\")\n",
    "    print(conv_layers_finetune)\n",
    "\n",
    "    print(f\"Freezing ({len(conv_layers_frozen)}) conv layers:\")\n",
    "    print(conv_layers_frozen)\n",
    "\n",
    "\n",
    "### SETTINGS ###\n",
    "\n",
    "class ConfigObject:\n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "    def to_dict(self):\n",
    "        return {key: getattr(self, key) for key in dir(self) if not key.startswith(\"__\")}\n",
    "\n",
    "# Example usage:\n",
    "args = ConfigObject(\n",
    "    initialization = 'pretrained_models/resnet26_pretrained.t7',\n",
    "    n_epochs = 110,\n",
    "    lr = 0.1,\n",
    "    lr_agent = 0.01,\n",
    "    batch_size_train = 128,\n",
    "    batch_size_test = 256,\n",
    "    spottune = False,\n",
    "    train_fraction = 0.05,\n",
    "    log_interval = 10,\n",
    "    gpu_idx = 0\n",
    ")\n",
    "\n",
    "wandb.init(\n",
    "    project='targeted-generalization',\n",
    "    config=args.to_dict()\n",
    ")\n",
    "\n",
    "n_classes = 12 # visda\n",
    "net = get_model(\"resnet26\", n_classes, dataset=args.initialization)\n",
    "setup_network(net)\n",
    "net = net.cuda()\n",
    "\n",
    "train_loader, val_loader, test_loader = get_visda_dataloaders(\n",
    "    train_dir='/export/r32/data/visda17/train',\n",
    "    val_dir='/export/r32/data/visda17/test',\n",
    "    test_dir='/export/r32/data/visda17/test',\n",
    "    batch_size_train=args.batch_size_train,\n",
    "    batch_size_test=args.batch_size_test,\n",
    "    train_fraction=args.train_fraction\n",
    ")\n",
    "\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters()),\n",
    "                          lr=args.lr,\n",
    "                          momentum=0.9,\n",
    "                          weight_decay=1e-4)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.n_epochs)\n",
    "\n",
    "if args.spottune:\n",
    "    agent = agent_net.resnet(sum(net.layer_config) * 2)\n",
    "    agent_optimizer = optim.SGD(agent.parameters(), \n",
    "        lr= args.lr_agent,\n",
    "        momentum=0.9, \n",
    "        weight_decay=0.001) \n",
    "    agent_scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        agent_optimizer,\n",
    "        T_max=args.n_epochs)\n",
    "else:\n",
    "    agent = None\n",
    "    agent_optimizer = None\n",
    "    agent_scheduler = None\n",
    "\n",
    "validate_epoch(val_loader, net, agent)\n",
    "for epoch in trange(args.n_epochs):\n",
    "    train_epoch(train_loader, net, agent)\n",
    "    validate_epoch(val_loader, net, agent)\n",
    "\n",
    "    wandb.log({'net_lr': scheduler.get_last_lr()})\n",
    "    scheduler.step()\n",
    "\n",
    "    if args.spottune:\n",
    "        wandb.log({'agent_lr': agent_scheduler.get_last_lr()})\n",
    "        agent_scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
